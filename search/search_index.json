{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello!","text":"<p>I am an experienced AI engineer and consultant with background in industry research labs and applied AI produts. I specialize in bridging the gap between cutting-edge AI research and practical solutions, with experience ranging from big tech to early-stage startups. I'm based in NYC.</p> <p>Looking for collaboration or need advice? Let\u2019s connect! </p> <p> </p> <p>Schedule a call?</p> <p>Read my blogposts</p> <p>Subscribe</p>"},{"location":"#some-projects-ive-been-tinkering-with-lately","title":"Some Projects I've Been Tinkering With Lately","text":"<p>ActionWeaver: An open source framework for building LLM applications that simplifies function calling with LLMs. View on GitHub </p> <p>AutoCoder:A weekend side project: a description-to-pull-request bot that answers questions and makes code changes in GitHub repositories using natural language instructions. Built with ActionWeaver and powered by LLM function calling. GitHub Repository | Watch Demo</p> <p>AI Agent for immigration lawyer I built a demo for a startup showcasing an end-to-end workflow to automate tasks for immigration lawyers, streamlining the visa application process. Watch Demo</p> <p>An Empirical Study of Neural Network Training Dynamics: An attempt to uncover what\u2019s really happening during neural network training. Read the blogpost</p> <p></p> <p>When does loss-based prioritization fail?: A research paper from my time at Uber, exploring methods to robustly accelerate neural network training. Arxiv</p> <p>Geospatial Monitoring at Scale: A project while working at Uber - a real-time system that monitors on traffic patterns everywhere Uber operates with Apache Flink and Complex Event Processing. I gave the presentation at Flink Forward San Francisco 2020 on behalf of Uber.  </p> <p>Watch Conference Talk (Flink Forward San Francisco 2020)</p>"},{"location":"writing-samples/","title":"Writing samples","text":"<p>Here are some writing samples of work that we\u2019ve done for a few different clients across the year</p> <ol> <li>Beating Proprietary Models With A Quick Fine Tune : We highlighted how you could use Modal to run grid search in parallel across more than 50 GPUs to find the best hyper-parameters to fine-tune an embedding model to beat OpenAI\u2019s text-embedding-3-small model</li> <li>Embedding English Wikipedia in under 15 minutes : We leveraged\u2019s Modal\u2019s cloud GPUs to embed the entirety of english wikipedia, showing how the platform was able to scale to arbitrarily high workloads with only a few lines of code  </li> <li>RAG is more than Vector Search : We highlighted some of the key advantages of using Timescale\u2019s platform to build out RAG applications - demonstrating how Raw SQL was a good fit for building out complex metadata filters that could combine lexical and vector search </li> <li>Enhancing Text-to-SQL with Synthetic Summaries : This article reframed text-to-sql as a retrieval task and we showed through a series of experiments using synthetic questions the benefit of using summaries against raw snippets. All of the code was done using Timescale to highlight how the platform was able to support these complex benchmarks with relative ease.</li> </ol> <p>Here are some writing samples from the instructor blog which we\u2019ve done</p> <ol> <li>Smarter Summaries w Finetuning GPT-3.5 and Chain Of Density - We were one of the first to show how fine-tuning GPT-3.5 could result in massive performance boosts for specific tasks. This post went viral and charted at #3 on Hackernews when we published it.</li> </ol> <p></p> <ol> <li> <p>Bad Schemas Could break Your LLM Structured Outputs - This was a research style article where we highlighted the dangers of using JSON mode versus structured outputs through a series of benchmarks that we performed on GPT-4o and Claude models against the GSM8k</p> </li> <li> <p>RAG Is More Than Embeddings Search - This was an article we put out last year talking about the limitations of embedding search and why scaling beyond a simple demo requires careful and thoughtful consideration to metadata filters and query understanding. </p> </li> </ol>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/","title":"Why Character Artists Are Embracing AI (And Why You Should Too)","text":"","tags":["diffusion","AI"]},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/#the-rapid-growth-of-the-ip-character-market","title":"The Rapid Growth of the IP Character Market","text":"<p>The IP character market is rapidly growing, and we all know the character entertainment market is massive. According to Overnight lines, mall fights and instant sellouts: Labubu toy mania comes to America Pop Mart reported record revenue of $638.5 million for the first half of 2024, a 62% increase over the same period a year earlier. Sales in its burgeoning North America segment totaled $24.9 million.</p> <p>You\u2019ve likely seen some of Pop Mart\u2019s famous IP characters, such as Labubu, Molly, and Bobo &amp; Coco. These IP characters are driving the creation of numerous derivatives, including toys, NFTs, avatars, digital twins, virtual pets, and 3D-printed artifacts.</p>","tags":["diffusion","AI"]},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/#ai-to-release-trapped-potential-make-your-character-ip-generate-its-full-value","title":"AI to Release Trapped Potential: Make Your Character IP Generate Its Full Value","text":"<p>As a character artist or IP creator, you've poured countless hours into perfecting your signature character, but your character's potential to generate income is severely limited by how quickly you can create new content.</p> <p>Reality:</p> <ul> <li>Fans constantly demand fresh content featuring your character, but manually creating each variation takes hours</li> <li>Major licensing opportunities require extensive character assets that would take months to produce by hand</li> <li>Other creators are scaling their character businesses while you're stuck in the time-consuming loop of manual creation</li> <li>Potential revenue is slipping away because you can't keep up with market demands or new trend</li> </ul> <p>But your character can now exist in infinite worlds, poses, and scenarios - without sacrificing your artistic vision or spending endless hours at the drawing board. Now with the help of AI, you can</p> <ul> <li>Generate hundreds of on-brand character variations in different styles and scenarios</li> <li>Create consistent merchandise-ready assets in minutes instead of days</li> <li>Scale your licensing potential by rapidly producing customized character content</li> <li>Maintain full creative control while dramatically increasing your output</li> <li>Add interactivity and emotional engagement, gamifying your IP characters</li> </ul> <p>This isn't about replacing your creativity - it's about amplifying it. Your character could appear in any setting, wearing any outfit, with any expression - all while maintaining your signature style.</p>","tags":["diffusion","AI"]},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/#using-labubu-as-an-example-generating-variations-with-stable-diffusion-models","title":"Using Labubu as an Example: Generating Variations with Stable Diffusion Models","text":"<p>With its iconic and whimsical design, Labubu is the perfect subject for creating diverse variations. Using Stable Diffusion models, we can effortlessly produce multiple variations of it like below:</p> <p></p> <p>Prompt: Vibrant, high-definition image of Labubu, the whimsical cartoon character, in an energetic dance pose, dressed in a futuristic streetwear ensemble. They're wearing an oversized holographic bomber jacket with iridescent reflections, a cropped white t-shirt underneath with binary code patterns, and baggy cargo pants with multiple neon-accented pockets. Their signature snapback cap with 'GEN AI' emblazoned across the front sits playfully tilted, working harmoniously with their bunny-like ears that peek out beneath. Neon-colored sneakers with glowing soles complete the look, matching their energetic dance moves. They're standing on their tiptoes, body swaying with dynamic movement, microphone gripped confidently in their small hands. Their signature round eyes sparkle with enthusiasm beneath the hat's brim. The character is captured mid-performance, with their expressive face showing pure joy. Their posture suggests movement and energy, with one arm raised high holding the microphone while the other arm is bent at their side. The lighting creates a stage-like atmosphere, highlighting the metallic and holographic elements of their outfit while casting a gentle glow on their features. Wearing Yeezy Foam Runner.</p> <p></p> <p>Prompt: Vibrant, high-definition image of Labubu, the whimsical cartoon character, coolly piloting a miniaturized Tesla Cybertruck scaled to their size. They're sporting sleek black aviator sunglasses that add an extra layer of attitude while perfectly framing their signature round eyes beneath the dark lenses. A black leather jacket completes their cool aesthetic, creating a striking contrast with their playful features. Their characteristic bunny ears just graze the angular roof of the tiny vehicle, which stands about three Labubus tall. The micro Cybertruck maintains all of Tesla's iconic design elements: the sharp-edged stainless steel exoskeleton, triangular roof profile, and continuous LED light bar, all adorably miniaturized. Inside the angular cabin, Labubu sits confidently at the minimal dashboard, one hand on the steering yoke while the other waves a small American flag out of the geometric window. The sunlight plays dramatically off both the Cybertruck's metallic exterior and Labubu's sunglasses, creating an interplay of reflections that emphasizes their cool, confident demeanor. The leather jacket adds a sophisticated sheen, completing the image of a tiny, stylish maverick at the wheel of their futuristic ride.</p>","tags":["diffusion","AI"]},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/#how-to-do-it","title":"How to do it","text":"<p>We use ComfyUI, a node-based interface designed for working with diffusion models, and the popular Flux dev model. However, the Flux model does not inherently recognize concepts like Labubu or your IP character. To incorporate this information into the model, we utilize a technique known as LoRA.</p> <p>Think of a model as a machine with numerous knobs. LoRA allows us to adjust only a small subset of these knobs, rather than fine-tuning the entire diffusion model (which is computationally expensive). This approach focuses on teaching the model specifically how our character looks. However, this step typically requires the most manual effort without automation tools. For more technically inclined readers, I recommend using FluxGym for fine-tuning. Additionally, you can find a helpful tutorial on creating LoRA on Civitai.</p> <p>Once the LoRA checkpoint is ready, you can use this workflow below (download the image and drag it into ComfyUI directly) to generate custom versions of your character with prompts. You can create multi-view representations of the character and even 3D models. Please note that custom nodes (e.g. ComfyUI-Easy-Use, ComfyUI-MVAdapter) must be installed to successfully run the workflow.</p> <p></p>","tags":["diffusion","AI"]},{"location":"blog/2025/01/13/why-character-artists-are-embracing-ai-and-why-you-should-too/#want-to-learn-more-about-this-topic","title":"\u2728 Want to learn more about this topic? \u2728","text":"<p>I'm available for free consultations, contract work, or commission projects. \ud83d\udcbc Shoot me an email: hu.niel92@gmail.com</p> <p>Subscribe to my newsletter or Schedule a call to connect!</p>","tags":["diffusion","AI"]},{"location":"blog/2022/08/02/training-dynamics/","title":"An Empirical Study of Neural Network Training Dynamics","text":"<p>In the paper Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications, the authors proposed several metrics to quantify examples by how well-represented they are in the underlying distribution.</p> <p>After reading the paper, I started wondering: When humans learn, we typically begin with easy materials and questions, gradually progressing to more difficult topics. Does neural network learning follow a similar pattern? Do networks learn easy or well-represented examples first and move on to more complex ones later?</p> <p>To address the question, I trained a simple fully-connected neural network with a single hidden layer consisting of 8 units on the MNIST dataset. To ensure more reliable and less noisy observations, I incorporated an evaluation step after each iteration. During this step, the network processed the entire training set, performed backpropagation, and recorded the prediction results and gradients of the second fully connected layer (an 8 \u00d7 10 matrix) for every example, without updating the network parameters.</p> <p>Since feeding the entire MNIST dataset to the network at every iteration significantly increases computational cost, I conducted the experiment using a randomly sampled mini-MNIST dataset, consisting of only 10% of the original MNIST data (6000 examples).</p> <p>The model was trained using vanilla stochastic gradient descent for 30 epochs, with a batch size of 128 and a learning rate of 0.01. The experiment was repeated ten times using different random seeds.</p> <p>The average training accuracy is summarized below. As expected, accuracy surpassed 80% within the first 100 iterations and gradually improved thereafter.</p> <p></p> <p>The next step is to produce the prototypicality ranking described by the paper. I chose to produce the ranking by applying the Ensemble Agreement(AGR) proposed in the paper, which computes symmetric JS-divergence for each example among ten models\u2019 predictions.</p> <p> </p> <p>(Left: 64 random examples. Middle: top 64 typical examples. Right: top 64 atypical examples)</p> <p>All examples were divided into ten chunks by their AGR scores, the first chunk being the most well-represented and the last chunk being the least well-represented. Their training accuracies are shown below.</p> <p></p> <p>Interestingly, even the simplest neural network training inherently follows an implicit curriculum: learning begins with typical, well-represented examples and gradually progresses to more challenging ones. For instance, most well-represented examples (chunk 0) are almost entirely learned within the first 50 iterations, whereas the accuracy for chunk 9 improves at a much slower pace.</p>"},{"location":"blog/2022/08/02/training-dynamics/#why-do-neural-networks-learn-the-easy-examples-first","title":"Why do neural networks learn the \u201ceasy\u201d examples first","text":"<p>How does such a curriculum naturally emerge during stochastic gradient descent training? Why are examples with varying levels of prototypicality treated differently?</p> <p>To explore this, I calculated the cosine similarity between the gradient of each individual example (from the evaluation step) and the average gradient across all examples. The plot below illustrates the results: the X-axis represents the number of iterations, and the Y-axis shows the average cosine similarity between  example's gradient and the overall average gradient for the mini-MNIST dataset, for each chunk.</p> <p></p> <p>The results show that the most well-represented chunk has the highest cosine similarity score, particularly during the first 50 iterations. This aligns with the observation that these examples benefited the most from training and achieved higher accuracy more quickly.</p> <p>On closer visual inspection, it\u2019s evident that \"well-represented\" examples share many similarities, leading to more similar gradient vectors. When aggregated, these vectors form a stronger overall gradient direction, effectively dominating the gradient descent process.</p> <p>Additionally, well-represented examples tend to have smaller gradients, as their associated losses are generally lower.</p> <p></p> <p>average l2 norm per chunk ranked by prototypicality</p>"},{"location":"blog/2022/08/02/training-dynamics/#well-represented-examples-are-less-prone-to-forgetting","title":"Well-represented examples are less prone to forgetting","text":"<p>In addition to the finding that examples of well-representation are learned faster, they are also less forgotten by the network once they\u2019re learned. Here, I defined two new metrics in addition to the accuracy:</p> <ul> <li> <p>First Correct (FC): The percentage of examples for which the model achieves at least one correct classification up to and including the current iteration.</p> </li> <li> <p>Last Mistake (LM): The percentage of examples for which the model makes no incorrect classifications from the current iteration onward, including the current iteration.</p> </li> </ul> <p>By definition, FC and LM are both monotonically increasing, and FC is always greater or equal to the actual prediction accuracy, LM is always less than or equal to the prediction accuracy.</p> <p>Using the definitions above, I created three plots for the top 10%, middle 10%, and bottom 10% of training examples ranked by AGR.</p> <p>The blue line is FC, the green line is training accuracy, and the orange line is LM. The region between the FC and LM lines is particularly interesting, as it represents examples that the network has correctly classified before but is still struggling to fully learn.</p> <p></p> <p>top 10% examples ranked by AGR</p> <p>The plot above highlights the top 10% of well-represented examples that were learned very early in the training process and were never forgotten afterward! </p> <p>middle 10% examples ranked by AGR</p> <p>As examples become less well-represented or more challenging, the envelope region expands, indicating that the network begins forgetting more of the early learned results as training progresses.</p> <p></p> <p>bottom 10% examples ranked by AGR</p> <p>The plot above shows the accuracy for the bottom 10% of examples, along with the large region between FC and LM. This suggests that, although the network is making some progress, it fails to \u201csecure\u201d these learnings.</p> <p>From the previous section, we know these examples are slow to learn because their atypical gradients are large and less aligned with the rest of the batch. But why do they also suffer from forgetting? Could this forgetting be caused by atypical gradients canceling each other out?</p>"},{"location":"blog/2022/08/02/training-dynamics/#visualizing-individual-example-gradients","title":"Visualizing individual example gradients","text":"<p>Using the individual gradients collected during training, I visualized them in a plot. The X-axis represents the gradient norm, while the Y-axis represents the cosine similarity between an example\u2019s gradient and the average gradient of the dataset.  </p> <p>Each point corresponds to an example: green indicates correct classification by the network, and red indicates incorrect classification.</p> <p></p> <p>iteration 0</p> <p>At the beginning (iteration 0), only a few examples (10% ?) were correctly classified by the randomly initialized network by luck. As the training progressed, more examples were correctly classified, and green points were moving to the left side of the plot because of low gradient norms, while the red points with large gradients moved to the right side. See here for the full video.</p> <p></p> <p>iteration 0 - 120</p> <p>Training is chaotic, but still, we\u2019re able to have some interesting observations:</p>"},{"location":"blog/2022/08/02/training-dynamics/#cluster-of-gradients","title":"Cluster of gradients","text":"<p>It\u2019s evident that some examples\u2019 gradients consistently move together, likely due to strong similarities among these examples. By identifying \"cones\" where all examples fall within a cosine similarity of 0.95 with each other, we can isolate clusters. This approach allows us to zoom in on small groups of examples, revealing distinct styles and nuances between them, even within the same class.</p> <p>Here are some example clusters identified by similar gradients:</p> <p></p> <p></p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#oscillation-and-examples-in-contention","title":"Oscillation and examples in contention","text":"<p>Looking closer, particularly after iteration 30, we can observe some clusters oscillating between positive and negative cosine similarity.  </p> <p>My initial hypothesis is that this oscillation arises from two groups of examples with gradients that counteract each other. Gradients from group 1 are initially larger, allowing them to dominate the batch and benefit from stochastic gradient descent, leading to improved accuracy for these examples. Simultaneously, another group (group 2) is negatively impacted by the gradient updates, causing their accuracy to drop and their losses and gradients to grow. Eventually, group 2's gradients surpass those of group 1, shifting the balance and allowing them to dominate the batch. This back-and-forth process repeats, resembling the dynamics of rotating binary stars.  </p> <p></p> <p>We could find those clusters using the \u201ccone\u201d cover technique mentioned above. For each cluster, we can also find examples learning against them by identifying examples under -0.95 cosine similarities. This reveals example clusters that are \u201cfighting\u201d against each other during training; This is interesting as it can give us insights into what kind of examples the network is struggling to learn and visually examine.</p> <p>Contention between \"2\"s and \"1\"s:</p> <p></p> <p>It makes sense the network confuses these two groups of examples from a visual perspective, and we can see symmetry in the cosine similarity plot and accuracy plot where one group\u2019s gain is another group\u2019s loss.</p> <p></p> <p>cosine similarly between each group and the average gradient of the entire mini-MNIST</p> <p></p> <p>average prediction accuracy of each group</p> <p>Contention between \"2\"s and \"6\"s:</p> <p></p> <p>This observation is intriguing, as the 6s (right) resemble mirrored versions of the 2s (left). This similarity might suggest that the network struggles to learn distinct and accurate representations for both digits.</p> <p>Contention between \"9\"s and \"4\"s:</p> <p></p> <p>Contention between \"7\"s and \"9\"s:</p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#visualize-gradients-in-networks-with-more-than-one-hidden-layer","title":"Visualize gradients in networks with more than one hidden layer","text":"<p>All these visualizations above are from networks with single hidden layers; we could apply the same visualization technique to networks with slightly more complicated architecture as well.</p> <p>I trained a network with four fully-connected layers (fc1: 28*28 -&gt; 32, fc2: 32 -&gt; 16, fc3: 16 -&gt; 10, fc4: 10 -&gt; 10)</p> <p>and applied the same visualization technique on fc2, fc3, and fc4. See here for the full video.</p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#conclusion","title":"Conclusion","text":"<p>The simplest neural network training demonstrates an implicit curriculum: well-represented examples are learned much faster and are less likely to be forgotten. The per-example gradient visualization uncovers fascinating phenomena, such as gradient clusters and the contention between two groups of examples that the network struggles to learn.  </p> <p>While this experiment focuses on a very basic neural network, I hope it provides insights and sparks ideas about the complex and often chaotic training dynamics of deep neural networks.</p>"},{"location":"blog/2022/08/02/training-dynamics/#acknowledgments","title":"Acknowledgments","text":"<p>I would like to thank Jason Yosinski, Rosanne Liu, and Sara Hooker for their guidance and support, and the ML Collective community for the comments and discussions.</p> <p>Link to Original Post on Medium</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2022/","title":"2022","text":""}]}