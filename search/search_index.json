{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello!","text":"<p>I am an experienced AI engineer and consultant with background in industry research labs and applied AI produts. I specialize in bridging the gap between cutting-edge AI research and practical solutions, with experience ranging from big tech to early-stage startups. I'm based in NYC.</p> <p>Looking for collaboration or need advice? Let\u2019s connect! </p> <p> </p> <p>Schedule a call?</p> <p>Read my blogposts</p> <p>Subscribe</p>"},{"location":"#some-projects-ive-been-tinkering-with-lately","title":"Some Projects I've Been Tinkering With Lately","text":"<p>ActionWeaver: An open source framework for building LLM applications that simplifies function calling with LLMs. View on GitHub </p> <p>AutoCoder:A weekend side project: a description-to-pull-request bot that answers questions and makes code changes in GitHub repositories using natural language instructions. Built with ActionWeaver and powered by LLM function calling. GitHub Repository | Watch Demo</p> <p>AI Agent for immigration lawyer I built a demo for a startup showcasing an end-to-end workflow to automate tasks for immigration lawyers, streamlining the visa application process. Watch Demo</p> <p>An Empirical Study of Neural Network Training Dynamics: An attempt to uncover what\u2019s really happening during neural network training. Read the blogpost</p> <p></p> <p>When does loss-based prioritization fail?: A research paper from my time at Uber, exploring methods to robustly accelerate neural network training. Arxiv</p> <p>Geospatial Monitoring at Scale: A project while working at Uber - a real-time system that monitors on traffic patterns everywhere Uber operates with Apache Flink and Complex Event Processing. I gave the presentation at Flink Forward San Francisco 2020 on behalf of Uber.  </p> <p>Watch Conference Talk (Flink Forward San Francisco 2020)</p>"},{"location":"writing-samples/","title":"Writing samples","text":"<p>Here are some writing samples of work that we\u2019ve done for a few different clients across the year</p> <ol> <li>Beating Proprietary Models With A Quick Fine Tune : We highlighted how you could use Modal to run grid search in parallel across more than 50 GPUs to find the best hyper-parameters to fine-tune an embedding model to beat OpenAI\u2019s text-embedding-3-small model</li> <li>Embedding English Wikipedia in under 15 minutes : We leveraged\u2019s Modal\u2019s cloud GPUs to embed the entirety of english wikipedia, showing how the platform was able to scale to arbitrarily high workloads with only a few lines of code  </li> <li>RAG is more than Vector Search : We highlighted some of the key advantages of using Timescale\u2019s platform to build out RAG applications - demonstrating how Raw SQL was a good fit for building out complex metadata filters that could combine lexical and vector search </li> <li>Enhancing Text-to-SQL with Synthetic Summaries : This article reframed text-to-sql as a retrieval task and we showed through a series of experiments using synthetic questions the benefit of using summaries against raw snippets. All of the code was done using Timescale to highlight how the platform was able to support these complex benchmarks with relative ease.</li> </ol> <p>Here are some writing samples from the instructor blog which we\u2019ve done</p> <ol> <li>Smarter Summaries w Finetuning GPT-3.5 and Chain Of Density - We were one of the first to show how fine-tuning GPT-3.5 could result in massive performance boosts for specific tasks. This post went viral and charted at #3 on Hackernews when we published it.</li> </ol> <p></p> <ol> <li> <p>Bad Schemas Could break Your LLM Structured Outputs - This was a research style article where we highlighted the dangers of using JSON mode versus structured outputs through a series of benchmarks that we performed on GPT-4o and Claude models against the GSM8k</p> </li> <li> <p>RAG Is More Than Embeddings Search - This was an article we put out last year talking about the limitations of embedding search and why scaling beyond a simple demo requires careful and thoughtful consideration to metadata filters and query understanding. </p> </li> </ol>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2022/08/02/training-dynamics/","title":"An Empirical Study of Neural Network Training Dynamics","text":"<p>In the paper Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications, the authors proposed several metrics to quantify examples by how well-represented they are in the underlying distribution.</p> <p>After reading the paper, I started wondering: When humans learn, we typically begin with easy materials and questions, gradually progressing to more difficult topics. Does neural network learning follow a similar pattern? Do networks learn easy or well-represented examples first and move on to more complex ones later?</p> <p>To address the question, I trained a simple fully-connected neural network with a single hidden layer consisting of 8 units on the MNIST dataset. To ensure more reliable and less noisy observations, I incorporated an evaluation step after each iteration. During this step, the network processed the entire training set, performed backpropagation, and recorded the prediction results and gradients of the second fully connected layer (an 8 \u00d7 10 matrix) for every example, without updating the network parameters.</p> <p>Since feeding the entire MNIST dataset to the network at every iteration significantly increases computational cost, I conducted the experiment using a randomly sampled mini-MNIST dataset, consisting of only 10% of the original MNIST data (6000 examples).</p> <p>The model was trained using vanilla stochastic gradient descent for 30 epochs, with a batch size of 128 and a learning rate of 0.01. The experiment was repeated ten times using different random seeds.</p> <p>The average training accuracy is summarized below. As expected, accuracy surpassed 80% within the first 100 iterations and gradually improved thereafter.</p> <p></p> <p>The next step is to produce the prototypicality ranking described by the paper. I chose to produce the ranking by applying the Ensemble Agreement(AGR) proposed in the paper, which computes symmetric JS-divergence for each example among ten models\u2019 predictions.</p> <p> </p> <p>(Left: 64 random examples. Middle: top 64 typical examples. Right: top 64 atypical examples)</p> <p>All examples were divided into ten chunks by their AGR scores, the first chunk being the most well-represented and the last chunk being the least well-represented. Their training accuracies are shown below.</p> <p></p> <p>Interestingly, even the simplest neural network training inherently follows an implicit curriculum: learning begins with typical, well-represented examples and gradually progresses to more challenging ones. For instance, most well-represented examples (chunk 0) are almost entirely learned within the first 50 iterations, whereas the accuracy for chunk 9 improves at a much slower pace.</p>"},{"location":"blog/2022/08/02/training-dynamics/#why-do-neural-networks-learn-the-easy-examples-first","title":"Why do neural networks learn the \u201ceasy\u201d examples first","text":"<p>How does such a curriculum naturally emerge during stochastic gradient descent training? Why are examples with varying levels of prototypicality treated differently?</p> <p>To explore this, I calculated the cosine similarity between the gradient of each individual example (from the evaluation step) and the average gradient across all examples. The plot below illustrates the results: the X-axis represents the number of iterations, and the Y-axis shows the average cosine similarity between  example's gradient and the overall average gradient for the mini-MNIST dataset, for each chunk.</p> <p></p> <p>The results show that the most well-represented chunk has the highest cosine similarity score, particularly during the first 50 iterations. This aligns with the observation that these examples benefited the most from training and achieved higher accuracy more quickly.</p> <p>On closer visual inspection, it\u2019s evident that \"well-represented\" examples share many similarities, leading to more similar gradient vectors. When aggregated, these vectors form a stronger overall gradient direction, effectively dominating the gradient descent process.</p> <p>Additionally, well-represented examples tend to have smaller gradients, as their associated losses are generally lower.</p> <p></p> <p>average l2 norm per chunk ranked by prototypicality</p>"},{"location":"blog/2022/08/02/training-dynamics/#well-represented-examples-are-less-prone-to-forgetting","title":"Well-represented examples are less prone to forgetting","text":"<p>In addition to the finding that examples of well-representation are learned faster, they are also less forgotten by the network once they\u2019re learned. Here, I defined two new metrics in addition to the accuracy:</p> <ul> <li> <p>First Correct (FC): The percentage of examples for which the model achieves at least one correct classification up to and including the current iteration.</p> </li> <li> <p>Last Mistake (LM): The percentage of examples for which the model makes no incorrect classifications from the current iteration onward, including the current iteration.</p> </li> </ul> <p>By definition, FC and LM are both monotonically increasing, and FC is always greater or equal to the actual prediction accuracy, LM is always less than or equal to the prediction accuracy.</p> <p>Using the definitions above, I created three plots for the top 10%, middle 10%, and bottom 10% of training examples ranked by AGR.</p> <p>The blue line is FC, the green line is training accuracy, and the orange line is LM. The region between the FC and LM lines is particularly interesting, as it represents examples that the network has correctly classified before but is still struggling to fully learn.</p> <p></p> <p>top 10% examples ranked by AGR</p> <p>The plot above highlights the top 10% of well-represented examples that were learned very early in the training process and were never forgotten afterward! </p> <p>middle 10% examples ranked by AGR</p> <p>As examples become less well-represented or more challenging, the envelope region expands, indicating that the network begins forgetting more of the early learned results as training progresses.</p> <p></p> <p>bottom 10% examples ranked by AGR</p> <p>The plot above shows the accuracy for the bottom 10% of examples, along with the large region between FC and LM. This suggests that, although the network is making some progress, it fails to \u201csecure\u201d these learnings.</p> <p>From the previous section, we know these examples are slow to learn because their atypical gradients are large and less aligned with the rest of the batch. But why do they also suffer from forgetting? Could this forgetting be caused by atypical gradients canceling each other out?</p>"},{"location":"blog/2022/08/02/training-dynamics/#visualizing-individual-example-gradients","title":"Visualizing individual example gradients","text":"<p>Using the individual gradients collected during training, I visualized them in a plot. The X-axis represents the gradient norm, while the Y-axis represents the cosine similarity between an example\u2019s gradient and the average gradient of the dataset.  </p> <p>Each point corresponds to an example: green indicates correct classification by the network, and red indicates incorrect classification.</p> <p></p> <p>iteration 0</p> <p>At the beginning (iteration 0), only a few examples (10% ?) were correctly classified by the randomly initialized network by luck. As the training progressed, more examples were correctly classified, and green points were moving to the left side of the plot because of low gradient norms, while the red points with large gradients moved to the right side. See here for the full video.</p> <p></p> <p>iteration 0 - 120</p> <p>Training is chaotic, but still, we\u2019re able to have some interesting observations:</p>"},{"location":"blog/2022/08/02/training-dynamics/#cluster-of-gradients","title":"Cluster of gradients","text":"<p>It\u2019s evident that some examples\u2019 gradients consistently move together, likely due to strong similarities among these examples. By identifying \"cones\" where all examples fall within a cosine similarity of 0.95 with each other, we can isolate clusters. This approach allows us to zoom in on small groups of examples, revealing distinct styles and nuances between them, even within the same class.</p> <p>Here are some example clusters identified by similar gradients:</p> <p></p> <p></p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#oscillation-and-examples-in-contention","title":"Oscillation and examples in contention","text":"<p>Looking closer, particularly after iteration 30, we can observe some clusters oscillating between positive and negative cosine similarity.  </p> <p>My initial hypothesis is that this oscillation arises from two groups of examples with gradients that counteract each other. Gradients from group 1 are initially larger, allowing them to dominate the batch and benefit from stochastic gradient descent, leading to improved accuracy for these examples. Simultaneously, another group (group 2) is negatively impacted by the gradient updates, causing their accuracy to drop and their losses and gradients to grow. Eventually, group 2's gradients surpass those of group 1, shifting the balance and allowing them to dominate the batch. This back-and-forth process repeats, resembling the dynamics of rotating binary stars.  </p> <p></p> <p>We could find those clusters using the \u201ccone\u201d cover technique mentioned above. For each cluster, we can also find examples learning against them by identifying examples under -0.95 cosine similarities. This reveals example clusters that are \u201cfighting\u201d against each other during training; This is interesting as it can give us insights into what kind of examples the network is struggling to learn and visually examine.</p> <p>Contention between \"2\"s and \"1\"s:</p> <p></p> <p>It makes sense the network confuses these two groups of examples from a visual perspective, and we can see symmetry in the cosine similarity plot and accuracy plot where one group\u2019s gain is another group\u2019s loss.</p> <p></p> <p>cosine similarly between each group and the average gradient of the entire mini-MNIST</p> <p></p> <p>average prediction accuracy of each group</p> <p>Contention between \"2\"s and \"6\"s:</p> <p></p> <p>This observation is intriguing, as the 6s (right) resemble mirrored versions of the 2s (left). This similarity might suggest that the network struggles to learn distinct and accurate representations for both digits.</p> <p>Contention between \"9\"s and \"4\"s:</p> <p></p> <p>Contention between \"7\"s and \"9\"s:</p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#visualize-gradients-in-networks-with-more-than-one-hidden-layer","title":"Visualize gradients in networks with more than one hidden layer","text":"<p>All these visualizations above are from networks with single hidden layers; we could apply the same visualization technique to networks with slightly more complicated architecture as well.</p> <p>I trained a network with four fully-connected layers (fc1: 28*28 -&gt; 32, fc2: 32 -&gt; 16, fc3: 16 -&gt; 10, fc4: 10 -&gt; 10)</p> <p>and applied the same visualization technique on fc2, fc3, and fc4. See here for the full video.</p> <p></p>"},{"location":"blog/2022/08/02/training-dynamics/#conclusion","title":"Conclusion","text":"<p>The simplest neural network training demonstrates an implicit curriculum: well-represented examples are learned much faster and are less likely to be forgotten. The per-example gradient visualization uncovers fascinating phenomena, such as gradient clusters and the contention between two groups of examples that the network struggles to learn.  </p> <p>While this experiment focuses on a very basic neural network, I hope it provides insights and sparks ideas about the complex and often chaotic training dynamics of deep neural networks.</p>"},{"location":"blog/2022/08/02/training-dynamics/#acknowledgments","title":"Acknowledgments","text":"<p>I would like to thank Jason Yosinski, Rosanne Liu, and Sara Hooker for their guidance and support, and the ML Collective community for the comments and discussions.</p> <p>Link to Original Post on Medium</p>"},{"location":"blog/2025/01/03/wealth-distribution-in-bubbles/","title":"Wealth redistribution in bubbles and crashes","text":"<p>This paper analyzed China's 2014-2015 stock market bubble, where rich investors made bank while smaller investors got burned.  The top 0.5% of investors pocketed 250 billion RMB, while 85% of smaller investors lost about 30% of their money. Wealth transfer from those who need it to those who don't. </p> <p>Three key patterns that feel similar to today's crypto market: Small investors were too cautious early in the bull run. They kept waiting and waiting, only jumping in when prices were already sky-high. The poorest investors actually pulled money out during the early bull market - big mistake. Mid-tier investors couldn't cut their losses. While the whales quickly dumped when prices started falling, smaller investors hesitated, sometimes even buying more to \"catch the falling knife.\" Once small investors finally jumped in, they went straight for the riskiest assets hoping to get rich quick. Usually ended up holding the bag.</p> <p>Looking at March 2024, we're seeing similar signs - zombie coins coming back to life, alt-coin indexes pumping... signs of retail FOMO.</p> <p>Two key takeaways: 1. Don't chase pumps 2. Stay away from alt-coins</p> <p></p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2022/","title":"2022","text":""}]}